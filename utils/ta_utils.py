from __future__ import annotations

import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
import math
import time
import logging
import threading
from typing import Dict, Optional, Tuple, List, Any, Callable, Union
from dataclasses import dataclass, field
from collections import defaultdict, OrderedDict
import functools
import inspect
from enum import Enum
from datetime import datetime, timedelta

# Performance monitoring için
import timeit
from contextlib import contextmanager

# Circuit breaker için
class CircuitState(Enum):
    CLOSED = "CLOSED"
    OPEN = "OPEN"
    HALF_OPEN = "HALF_OPEN"

# Config fallback mekanizması - GELİŞMİŞ
try:
    from utils.config import CONFIG
    CONFIG_LOADED = True
except ImportError as e:
    logging.warning(f"utils.config yüklenemedi, güvenli default değerler kullanılıyor: {e}")
    
    @dataclass
    class SafeConfig:
        class TA:
            EMA_PERIOD = 20
            MACD_FAST = 12
            MACD_SLOW = 26
            MACD_SIGNAL = 9
            ADX_PERIOD = 14
            RSI_PERIOD = 14
            STOCH_K = 14
            STOCH_D = 3
            ATR_PERIOD = 14
            BB_PERIOD = 20
            BB_STDDEV = 2.0
            SHARPE_RISK_FREE_RATE = 0.02
            SHARPE_PERIOD = 252
            TA_CACHE_TTL = 300
            TA_PIPELINE_INTERVAL = 60
            TA_MIN_DATA_POINTS = 20
            ALPHA_LONG_THRESHOLD = 0.6
            ALPHA_SHORT_THRESHOLD = -0.6
            KALMAN_Q = 1e-5
            KALMAN_R = 1e-2
            REGIME_WINDOW = 80
            ENTROPY_M = 3
            ENTROPY_R_FACTOR = 0.2
            LEADLAG_MAX_LAG = 10
            W_KALMAN = 0.20
            W_HILBERT = 0.20
            W_ENTROPY = 0.20
            W_REGIME = 0.20
            W_LEADLAG = 0.20
            EMA_PERIODS = [20, 50]
        
        class SYSTEM:
            MAX_WORKERS = 2
    
    CONFIG = SafeConfig()
    CONFIG_LOADED = False

# ------------------------------------------------------------
# Logger Kurulumu - GELİŞMİŞ
# ------------------------------------------------------------
logger = logging.getLogger("ta_utils")
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO if CONFIG_LOADED else logging.WARNING)

# ------------------------------------------------------------
# Performance Monitoring
# ------------------------------------------------------------
@contextmanager
def time_execution(operation_name: str):
    """Execution time tracking için context manager"""
    start_time = time.time()
    try:
        yield
    finally:
        end_time = time.time()
        execution_time = end_time - start_time
        logger.debug(f"{operation_name} executed in {execution_time:.4f} seconds")

def track_performance(func):
    """Fonksiyon execution time tracking decorator"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        with time_execution(f"Function {func.__name__}"):
            return func(*args, **kwargs)
    return wrapper

# ------------------------------------------------------------
# Circuit Breaker Pattern
# ------------------------------------------------------------
class CircuitBreaker:
    """TA fonksiyonları için circuit breaker pattern"""
    def __init__(self, failure_threshold: int = 5, reset_timeout: int = 60, name: str = "ta_default"):
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = CircuitState.CLOSED
        self.name = name
        self.success_count = 0
        logger.info(f"CircuitBreaker '{name}' initialized with threshold {failure_threshold}, timeout {reset_timeout}")

    def execute(self, func, *args, **kwargs):
        """Fonksiyonu circuit breaker ile çalıştır"""
        current_time = time.time()
        
        # Circuit OPEN durumunda ve timeout dolmadıysa
        if self.state == CircuitState.OPEN:
            if current_time - self.last_failure_time > self.reset_timeout:
                self.state = CircuitState.HALF_OPEN
                logger.warning(f"CircuitBreaker '{self.name}' moving to HALF_OPEN state")
            else:
                remaining = self.reset_timeout - (current_time - self.last_failure_time)
                logger.error(f"CircuitBreaker '{self.name}' is OPEN. Retry in {remaining:.1f}s")
                raise Exception(f"Circuit breaker is OPEN. Retry in {remaining:.1f}s")
        
        try:
            # Fonksiyonu çalıştır
            result = func(*args, **kwargs)
            
            # Başarılı ise state'i güncelle
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
                self.failure_count = 0
                self.success_count += 1
                logger.info(f"CircuitBreaker '{self.name}' reset to CLOSED state after successful execution")
            
            return result
            
        except Exception as e:
            # Hata durumunda circuit breaker state'ini güncelle
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                logger.error(f"CircuitBreaker '{self.name}' tripped to OPEN state due to {self.failure_count} failures")
            
            logger.error(f"CircuitBreaker '{self.name}' execution failed: {str(e)}")
            raise e

    def get_status(self) -> Dict[str, Any]:
        """Circuit breaker durumunu getir"""
        return {
            "state": self.state.value,
            "failure_count": self.failure_count,
            "success_count": self.success_count,
            "last_failure_time": self.last_failure_time,
            "time_since_last_failure": time.time() - self.last_failure_time if self.last_failure_time > 0 else 0
        }

# Global circuit breaker instances
ta_circuit_breaker = CircuitBreaker(
    failure_threshold=5,
    reset_timeout=60,
    name="ta_main"
)

# ------------------------------------------------------------
# İç yardımcılar ve Cache Mekanizması - GELİŞMİŞ
# ------------------------------------------------------------

def _get_max_workers(default: int = 2) -> int:
    """Thread sayısını güvenli şekilde alır"""
    try:
        if not CONFIG_LOADED:
            return default
            
        system = getattr(CONFIG, "SYSTEM", None)
        if system is not None and hasattr(system, "MAX_WORKERS"):
            try:
                mw = int(getattr(system, "MAX_WORKERS"))
                return max(1, min(mw, 10))  # 1-10 arası sınırla
            except (ValueError, TypeError):
                logger.warning("MAX_WORKERS geçersiz, default değer kullanılıyor")
                return default
        return default
    except Exception as e:
        logger.warning(f"MAX_WORKERS alınırken hata: {e}, default değer kullanılıyor: {default}")
        return default

@dataclass
class TAMetrics:
    """TA metriklerini tutan data class"""
    total_calculations: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    calculation_errors: int = 0
    io_requests: int = 0
    io_errors: int = 0
    execution_times: Dict[str, List[float]] = field(default_factory=lambda: defaultdict(list))

class AdaptiveCache:
    """Adaptive cache with automatic size adjustment"""
    def __init__(self, initial_max_size: int = 1000, min_size: int = 100, max_size: int = 5000):
        self._cache = OrderedDict()
        self._hit_count = 0
        self._miss_count = 0
        self._last_cleanup = time.time()
        self._min_size = min_size
        self._max_size = max_size
        self._current_max_size = initial_max_size
        self._lock = threading.RLock()
        self._access_pattern = []  # LRU pattern tracking
        self._last_optimization = time.time()
        logger.info(f"Adaptive Cache initialized with size range: {min_size}-{max_size}")
    
    def _optimize_size(self):
        """Cache boyutunu otomatik optimize et"""
        current_time = time.time()
        if current_time - self._last_optimization < 300:  # 5 dakikada bir
            return
            
        hit_rate = self._hit_count / max(self._hit_count + self._miss_count, 1)
        
        # Hit rate'e göre cache boyutunu ayarla
        if hit_rate > 0.8 and self._current_max_size < self._max_size:
            # Yüksek hit rate, cache boyutunu artır
            new_size = min(self._current_max_size * 1.2, self._max_size)
            logger.info(f"Optimizing cache: increasing size from {self._current_max_size} to {new_size} (hit rate: {hit_rate:.2%})")
            self._current_max_size = int(new_size)
        elif hit_rate < 0.3 and self._current_max_size > self._min_size:
            # Düşük hit rate, cache boyutunu azalt
            new_size = max(self._current_max_size * 0.8, self._min_size)
            logger.info(f"Optimizing cache: decreasing size from {self._current_max_size} to {new_size} (hit rate: {hit_rate:.2%})")
            self._current_max_size = int(new_size)
            
        self._last_optimization = current_time
    
    def get_ta_result(self, symbol: str, timeframe: str, indicator: str) -> Optional[Any]:
        """Cache'ten sonucu getir"""
        key = f"{symbol}_{timeframe}_{indicator}"
        with self._lock:
            self._optimize_size()  # Boyut optimizasyonu
            
            if key in self._cache:
                cache_data = self._cache[key]
                # LRU özelliği için en son kullanılanı en sona taşı
                self._cache.move_to_end(key)
                if time.time() < cache_data['expiry']:
                    self._hit_count += 1
                    logger.debug(f"Cache hit for {key}")
                    return cache_data['value']
                else:
                    # Süresi dolmuş entry'yi sil
                    del self._cache[key]
                    logger.debug(f"Cache expired for {key}")
            self._miss_count += 1
            return None
    
    def set_ta_result(self, symbol: str, timeframe: str, indicator: str, value: Any, ttl: int = 300):
        """Cache'e sonucu kaydet"""
        key = f"{symbol}_{timeframe}_{indicator}"
        with self._lock:
            # Cache limit kontrolü
            if len(self._cache) >= self._current_max_size:
                # LRU prensibiyle en eski entry'yi sil
                removed_key, _ = self._cache.popitem(last=False)
                logger.debug(f"Cache limit reached, removed: {removed_key}")
            
            self._cache[key] = {
                'value': value,
                'expiry': time.time() + ttl
            }
            # LRU özelliği için en son ekleneni en sona taşı
            self._cache.move_to_end(key)
            self._cleanup_expired()
    
    def _cleanup_expired(self):
        """Süresi dolmuş cache entry'lerini temizle"""
        current_time = time.time()
        if current_time - self._last_cleanup > 300:  # 5 dakikada bir temizle
            expired_keys = [k for k, v in self._cache.items() if v['expiry'] < current_time]
            for key in expired_keys:
                del self._cache[key]
            if expired_keys:
                logger.debug(f"Cache cleanup completed. Removed {len(expired_keys)} expired entries.")
            self._last_cleanup = current_time
    
    def get_stats(self) -> Dict[str, Any]:
        """Cache istatistiklerini getir"""
        with self._lock:
            total = self._hit_count + self._miss_count
            return {
                'size': len(self._cache),
                'hits': self._hit_count,
                'misses': self._miss_count,
                'hit_ratio': self._hit_count / max(total, 1),
                'max_size': self._current_max_size,
                'utilization': len(self._cache) / self._current_max_size,
                'min_size': self._min_size,
                'max_possible_size': self._max_size
            }
    
    def clear(self):
        """Cache'i tamamen temizle"""
        with self._lock:
            self._cache.clear()
            self._hit_count = 0
            self._miss_count = 0
            logger.info("TA Cache cleared completely")

# Global instances (thread-safe)
ta_cache = AdaptiveCache(initial_max_size=1000, min_size=100, max_size=5000)
ta_metrics = TAMetrics()

# ------------------------------------------------------------
# Unit Test Decorator
# ------------------------------------------------------------
def unit_test(expected_result=None, tolerance=0.01):
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            
            if expected_result is not None:
                try:
                    if isinstance(result, pd.Series):
                        if not result.dropna().empty:
                            last_val = result.dropna().iloc[-1]
                            if abs(last_val - expected_result) > tolerance:
                                logger.warning(
                                    f"Unit test failed for {func.__name__}: got {last_val}, expected {expected_result}"
                                )
                    elif isinstance(result, (list, tuple, np.ndarray)):
                        if len(result) > 0:
                            last_val = result[-1]
                            if abs(last_val - expected_result) > tolerance:
                                logger.warning(
                                    f"Unit test failed for {func.__name__}: got {last_val}, expected {expected_result}"
                                )
                    else:
                        if abs(result - expected_result) > tolerance:
                            logger.warning(
                                f"Unit test failed for {func.__name__}: got {result}, expected {expected_result}"
                            )
                except Exception as e:
                    logger.error(f"Unit test comparison failed: {e}")
            return result
        return wrapper
    return decorator

# =============================================================
# Yardımcı Fonksiyonlar - GELİŞMİŞ
# =============================================================

# Güncellenmiş validate_dataframe

def validate_dataframe(df: pd.DataFrame, required_columns: List[str] = None) -> bool:
    """DataFrame'in geçerli olup olmadığını kontrol et"""
    if df is None or df.empty:
        logger.warning("DataFrame is None or empty")
        return False
    
    if required_columns is None:
        required_columns = ['open', 'high', 'low', 'close', 'volume']
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.warning(f"DataFrame missing required columns: {missing_columns}")
        return False
    
    try:
        # NaN kontrolünü düzelt - Series yerine scalar boolean döndür
        nan_check = df[required_columns].isna().all()
        
        # Pandas Series için doğru boolean kontrolü
        if isinstance(nan_check, pd.Series):
            if nan_check.any():  # Herhangi bir sütunda tüm değerler NaN mi?
                logger.warning("All values are NaN in some columns")
                return False
        elif bool(nan_check):  # Scalar değer için
            logger.warning("All values are NaN in DataFrame")
            return False
            
        return True
    except Exception as e:
        logger.warning(f"NaN check failed: {e}")
        return False
        
def safe_column_access(df: pd.DataFrame, column: str, default_value: Any = np.nan, 
                      log_warning: bool = True) -> pd.Series:
    """Güvenli sütun erişimi sağlar"""
    if df is None or df.empty:
        if log_warning:
            logger.warning(f"DataFrame is None or empty, returning default for {column}")
        return pd.Series([default_value], index=[pd.Timestamp.now()] if df is None else df.index)
    
    if column in df.columns:
        series = df[column]
        if series.isna().all():
            if log_warning:
                logger.warning(f"All values are NaN in column {column}")
            return pd.Series([default_value] * len(df), index=df.index, name=column)
        return series
    else:
        if log_warning:
            logger.warning(f"Column {column} not found in DataFrame, returning default")
        return pd.Series([default_value] * len(df), index=df.index, name=column)

def safe_rolling_calculation(series: pd.Series, window: int, calculation: Callable, 
                           min_periods: int = 1, default: Any = np.nan) -> pd.Series:
    """Güvenli rolling hesaplama"""
    try:
        if len(series) < min_periods:
            return pd.Series([default] * len(series), index=series.index)
        return calculation(series, window)
    except Exception as e:
        logger.error(f"Rolling calculation failed: {e}")
        return pd.Series([default] * len(series), index=series.index)

def normalize_symbol(symbol: str) -> str:
    """Sembol ismini normalize et"""
    if not symbol:
        return "BTCUSDT"  # Default symbol
    
    # Çeşitli formatları standart formata dönüştür
    symbol = symbol.upper().replace('/', '').replace(':', '').replace('-', '')
    
    # USDT ile bitmiyorsa ekle (Binance standardı)
    if not symbol.endswith('USDT') and len(symbol) > 0:
        symbol += 'USDT'
    
    return symbol

# =============================================================
# Trend İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================
@track_performance
@unit_test(expected_result=104.5, tolerance=0.1)
def ema(df: pd.DataFrame, period: Optional[int] = None, column: str = "close") -> pd.Series:
    """Exponential Moving Average hesaplar."""
    try:
        # Daha güvenli validation kontrolü
        if df is None or df.empty:
            return pd.Series([np.nan], index=[pd.Timestamp.now()])
        
        if column not in df.columns:
            logger.warning(f"Column {column} not found in DataFrame")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        period = period or getattr(CONFIG.TA, 'EMA_PERIOD', 20)
        price_series = df[column].dropna()  # NaN değerleri temizle
        
        if len(price_series) < period:
            logger.warning(f"EMA: Not enough valid data points ({len(price_series)} < {period})")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        result = price_series.ewm(span=period, adjust=False).mean()
        
        # Orijinal index ile uyumlu hale getir
        return result.reindex(df.index, fill_value=np.nan)
        
    except Exception as e:
        logger.error(f"EMA hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index if df is not None else [])

@track_performance
def macd(df: pd.DataFrame, fast: Optional[int] = None, slow: Optional[int] = None, 
         signal: Optional[int] = None, column: str = "close") -> Tuple[pd.Series, pd.Series, pd.Series]:
    """
    MACD (Moving Average Convergence Divergence) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        fast: Fast EMA periyodu (default: CONFIG.TA.MACD_FAST veya 12)
        slow: Slow EMA periyodu (default: CONFIG.TA.MACD_SLOW veya 26)
        signal: Signal line periyodu (default: CONFIG.TA.MACD_SIGNAL veya 9)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        (macd_line, signal_line, histogram) tuple'ı
    """
    try:
        if not validate_dataframe(df, [column]):
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series, nan_series

        fast = fast or getattr(CONFIG.TA, 'MACD_FAST', 12)
        slow = slow or getattr(CONFIG.TA, 'MACD_SLOW', 26)
        signal = signal or getattr(CONFIG.TA, 'MACD_SIGNAL', 9)

        price_series = safe_column_access(df, column)
        
        # Yeterli veri kontrolü
        min_period = max(fast, slow, signal)
        if len(price_series) < min_period:
            logger.warning(f"MACD: Not enough data points ({len(price_series)} < {min_period})")
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series, nan_series

        ema_fast = price_series.ewm(span=fast, adjust=False).mean()
        ema_slow = price_series.ewm(span=slow, adjust=False).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal, adjust=False).mean()
        histogram = macd_line - signal_line

        return ta_circuit_breaker.execute(lambda: (macd_line, signal_line, histogram))
    except Exception as e:
        logger.error(f"MACD hesaplanırken hata: {e}")
        nan_series = pd.Series([np.nan] * len(df), index=df.index)
        return nan_series, nan_series, nan_series

@track_performance
# Güncellenmiş ADX
@track_performance
def adx(df: pd.DataFrame, period: Optional[int] = None) -> pd.Series:
    """Average Directional Index (ADX) hesaplar."""
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        period = period or getattr(CONFIG.TA, 'ADX_PERIOD', 14)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        
        # TR, +DM, -DM hesaplama
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        up_move = high - high.shift(1)
        down_move = low.shift(1) - low
        
        plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)
        minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)
        
        # Wilder's smoothing - pandas Series olarak
        tr_smooth = tr.ewm(alpha=1/period, adjust=False).mean()
        plus_di = 100 * pd.Series(plus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / tr_smooth
        minus_di = 100 * pd.Series(minus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / tr_smooth
        
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-12)
        adx_val = dx.ewm(alpha=1/period, adjust=False).mean()
        
        return ta_circuit_breaker.execute(lambda: adx_val)
    except Exception as e:
        logger.error(f"ADX hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def vwap(df: pd.DataFrame) -> pd.Series:
    """
    Volume Weighted Average Price (VWAP) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
    
    Returns:
        VWAP değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        volume = safe_column_access(df, 'volume')
        
        typical_price = (high + low + close) / 3
        cumulative_tp_volume = (typical_price * volume).cumsum()
        cumulative_volume = volume.cumsum()
        
        return ta_circuit_breaker.execute(lambda: cumulative_tp_volume / cumulative_volume)
    except Exception as e:
        logger.error(f"VWAP hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def cci(df: pd.DataFrame, period: int = 20) -> pd.Series:
    """
    Commodity Channel Index (CCI) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: CCI periyodu (default: 20)
    
    Returns:
        CCI değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        
        tp = (high + low + close) / 3
        sma = tp.rolling(window=period).mean()
        mad = tp.rolling(window=period).apply(lambda x: np.mean(np.abs(x - np.mean(x))), raw=True)
        
        return ta_circuit_breaker.execute(lambda: (tp - sma) / (0.015 * mad + 1e-12))
    except Exception as e:
        logger.error(f"CCI hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def momentum(df: pd.DataFrame, period: int = 10, column: str = "close") -> pd.Series:
    """
    Momentum Oscillator hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: Momentum periyodu (default: 10)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Momentum değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        price_series = safe_column_access(df, column)
        
        return ta_circuit_breaker.execute(lambda: price_series / price_series.shift(period) * 100)
    except Exception as e:
        logger.error(f"Momentum hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

# =============================================================
# Momentum İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================

@track_performance
def rsi(df: pd.DataFrame, period: Optional[int] = None, column: str = "close") -> pd.Series:
    """
    Relative Strength Index (RSI) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: RSI periyodu (default: CONFIG.TA.RSI_PERIOD veya 14)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        RSI değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        period = period or getattr(CONFIG.TA, 'RSI_PERIOD', 14)
        price_series = safe_column_access(df, column)
        
        if len(price_series) < period + 1:
            logger.warning(f"RSI: Not enough data points ({len(price_series)} < {period + 1})")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        delta = price_series.diff()
        gain = np.where(delta > 0, delta, 0)
        loss = np.where(delta < 0, -delta, 0)
        
        avg_gain = pd.Series(gain).rolling(window=period).mean()
        avg_loss = pd.Series(loss).rolling(window=period).mean()
        
        rs = avg_gain / (avg_loss + 1e-12)
        
        return ta_circuit_breaker.execute(lambda: 100 - (100 / (1 + rs)))
    except Exception as e:
        logger.error(f"RSI hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def stochastic(df: pd.DataFrame, k_period: Optional[int] = None, d_period: Optional[int] = None) -> Tuple[pd.Series, pd.Series]:
    """
    Stochastic Oscillator hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        k_period: %K periyodu (default: CONFIG.TA.STOCH_K veya 14)
        d_period: %D periyodu (default: CONFIG.TA.STOCH_D veya 3)
    
    Returns:
        (%K, %D) tuple'ı
    """
    try:
        if not validate_dataframe(df):
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series

        k_period = k_period or getattr(CONFIG.TA, 'STOCH_K', 14)
        d_period = d_period or getattr(CONFIG.TA, 'STOCH_D', 3)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        
        if len(close) < k_period:
            logger.warning(f"Stochastic: Not enough data points ({len(close)} < {k_period})")
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series
        
        lowest_low = low.rolling(window=k_period).min()
        highest_high = high.rolling(window=k_period).max()
        
        k = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-12)
        d = k.rolling(window=d_period).mean()
        
        return ta_circuit_breaker.execute(lambda: (k, d))
    except Exception as e:
        logger.error(f"Stochastic hesaplanırken hata: {e}")
        nan_series = pd.Series([np.nan] * len(df), index=df.index)
        return nan_series, nan_series

# =============================================================
# Volatilite & Risk İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================

@track_performance
def atr(df: pd.DataFrame, period: Optional[int] = None) -> pd.Series:
    """
    Average True Range (ATR) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: ATR periyodu (default: CONFIG.TA.ATR_PERIOD veya 14)
    
    Returns:
        ATR değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        period = period or getattr(CONFIG.TA, 'ATR_PERIOD', 14)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        return ta_circuit_breaker.execute(lambda: tr.rolling(window=period).mean())
    except Exception as e:
        logger.error(f"ATR hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def bollinger_bands(df: pd.DataFrame, period: Optional[int] = None, 
                   std_dev: Optional[float] = None, column: str = "close") -> Tuple[pd.Series, pd.Series, pd.Series]:
    """
    Bollinger Bands hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: BB periyodu (default: CONFIG.TA.BB_PERIOD veya 20)
        std_dev: Standart sapma çarpanı (default: CONFIG.TA.BB_STDDEV veya 2.0)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        (upper_band, middle_band, lower_band) tuple'ı
    """
    try:
        if not validate_dataframe(df, [column]):
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series, nan_series

        period = period or getattr(CONFIG.TA, 'BB_PERIOD', 20)
        std_dev = std_dev or getattr(CONFIG.TA, 'BB_STDDEV', 2.0)
        price_series = safe_column_access(df, column)
        
        if len(price_series) < period:
            logger.warning(f"Bollinger Bands: Not enough data points ({len(price_series)} < {period})")
            nan_series = pd.Series([np.nan] * len(df), index=df.index)
            return nan_series, nan_series, nan_series
        
        middle_band = price_series.rolling(window=period).mean()
        std = price_series.rolling(window=period).std()
        upper_band = middle_band + (std_dev * std)
        lower_band = middle_band - (std_dev * std)
        
        return ta_circuit_breaker.execute(lambda: (upper_band, middle_band, lower_band))
    except Exception as e:
        logger.error(f"Bollinger Bands hesaplanırken hata: {e}")
        nan_series = pd.Series([np.nan] * len(df), index=df.index)
        return nan_series, nan_series, nan_series

@track_performance
def sharpe_ratio(df: pd.DataFrame, risk_free_rate: Optional[float] = None, 
                period: Optional[int] = None, column: str = "close") -> pd.Series:
    """
    Sharpe Ratio hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        risk_free_rate: Risksiz faiz oranı (default: CONFIG.TA.SHARPE_RISK_FREE_RATE veya 0.02)
        period: Sharpe periyodu (default: CONFIG.TA.SHARPE_PERIOD veya 252)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Sharpe Ratio değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        risk_free_rate = risk_free_rate or getattr(CONFIG.TA, 'SHARPE_RISK_FREE_RATE', 0.02)
        period = period or getattr(CONFIG.TA, 'SHARPE_PERIOD', 252)
        price_series = safe_column_access(df, column)
        
        if len(price_series) < 2:
            logger.warning("Sharpe Ratio: Not enough data points for returns calculation")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        returns = price_series.pct_change()
        excess_returns = returns - (risk_free_rate / period)
        
        # Yıllıklaştırılmış Sharpe Ratio
        sharpe = (excess_returns.rolling(window=period).mean() / 
                 (excess_returns.rolling(window=period).std() + 1e-12)) * np.sqrt(period)
        
        return ta_circuit_breaker.execute(lambda: sharpe)
    except Exception as e:
        logger.error(f"Sharpe Ratio hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def max_drawdown(df: pd.DataFrame, column: str = "close") -> pd.Series:
    """
    Max Drawdown hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Max Drawdown değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        price_series = safe_column_access(df, column)
        
        rolling_max = price_series.expanding().max()
        drawdown = (price_series - rolling_max) / (rolling_max + 1e-12)
        
        return ta_circuit_breaker.execute(lambda: drawdown)
    except Exception as e:
        logger.error(f"Max Drawdown hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def historical_volatility(df: pd.DataFrame, period: int = 30, column: str = "close") -> pd.Series:
    """
    Historical Volatility (annualized, %) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: HV periyodu (default: 30)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Historical Volatility değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        price_series = safe_column_access(df, column)
        
        if len(price_series) < 2:
            logger.warning("Historical Volatility: Not enough data points for returns calculation")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        log_returns = np.log(price_series / price_series.shift(1))
        volatility = log_returns.rolling(window=period).std() * np.sqrt(252) * 100
        
        return ta_circuit_breaker.execute(lambda: volatility)
    except Exception as e:
        logger.error(f"Historical Volatility hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def ulcer_index(df: pd.DataFrame, period: int = 14, column: str = "close") -> pd.Series:
    """
    Ulcer Index hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: UI periyodu (default: 14)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Ulcer Index değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        price_series = safe_column_access(df, column)
        
        if len(price_series) < period:
            logger.warning(f"Ulcer Index: Not enough data points ({len(price_series)} < {period})")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        rolling_max = price_series.rolling(window=period).max()
        drawdown = (price_series - rolling_max) / (rolling_max + 1e-12) * 100
        ulcer_idx = np.sqrt((drawdown.pow(2)).rolling(window=period).mean())
        
        return ta_circuit_breaker.execute(lambda: ulcer_idx)
    except Exception as e:
        logger.error(f"Ulcer Index hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

# =============================================================
# Hacim & Likidite İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================

@track_performance
def obv(df: pd.DataFrame) -> pd.Series:
    """
    On-Balance Volume (OBV) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
    
    Returns:
        OBV değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        close = safe_column_access(df, 'close')
        volume = safe_column_access(df, 'volume')
        
        obv = (np.sign(close.diff()) * volume).fillna(0).cumsum()
        
        return ta_circuit_breaker.execute(lambda: obv)
    except Exception as e:
        logger.error(f"OBV hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def cmf(df: pd.DataFrame, period: int = 20) -> pd.Series:
    """
    Chaikin Money Flow (CMF) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: CMF periyodu (default: 20)
    
    Returns:
        CMF değerlerini içeren pandas Series
    """
    try:
        if not validate_dataframe(df):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        close = safe_column_access(df, 'close')
        volume = safe_column_access(df, 'volume')
        
        mfm = ((close - low) - (high - close)) / (high - low + 1e-12)
        mfv = mfm * volume
        
        cmf_val = mfv.rolling(window=period).sum() / volume.rolling(window=period).sum()
        
        return ta_circuit_breaker.execute(lambda: cmf_val)
    except Exception as e:
        logger.error(f"CMF hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index)

@track_performance
def order_book_imbalance(bids: list, asks: list) -> float:
    """
    Order Book Imbalance (OBI) hesaplar.
    
    Args:
        bids: Bid fiyat ve miktar listesi [(price, quantity), ...]
        asks: Ask fiyat ve miktar listesi [(price, quantity), ...]
    
    Returns:
        Order Book Imbalance değeri (-1 ile 1 arasında)
    """
    try:
        if not bids or not asks:
            logger.warning("Order Book Imbalance: Empty bids or asks")
            return 0.0
        
        bid_vol = sum([b[1] for b in bids]) if bids else 0
        ask_vol = sum([a[1] for a in asks]) if asks else 0
        denom = (bid_vol + ask_vol) or 1e-12
        
        return ta_circuit_breaker.execute(lambda: (bid_vol - ask_vol) / denom)
    except Exception as e:
        logger.error(f"Order Book Imbalance hesaplanırken hata: {e}")
        return 0.0

@track_performance
def open_interest_placeholder() -> None:
    """
    Open Interest için placeholder (borsa API ile entegre edilecek).
    
    Returns:
        None
    """
    logger.warning("Open Interest placeholder called - implement with exchange API")
    return None

# =============================================================
# Market Structure İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================

@track_performance
def market_structure(df: pd.DataFrame) -> pd.DataFrame:
    """
    Market Structure High/Low (MSH/MSL) hesaplar.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
    
    Returns:
        Market Structure bilgilerini içeren DataFrame
    """
    try:
        if not validate_dataframe(df):
            return pd.DataFrame(index=df.index if df is not None else [])
        
        high = safe_column_access(df, 'high')
        low = safe_column_access(df, 'low')
        
        structure = pd.DataFrame(index=df.index)
        structure['higher_high'] = high > high.shift(1)
        structure['lower_low'] = low < low.shift(1)
        
        return ta_circuit_breaker.execute(lambda: structure)
    except Exception as e:
        logger.error(f"Market Structure hesaplanırken hata: {e}")
        return pd.DataFrame(index=df.index if df is not None else [])

@track_performance
def breakout(df: pd.DataFrame, period: int = 20, column: str = "close") -> pd.Series:
    """
    Breakout (Donchian Channel tarzı) sinyal döndürür.
    
    Args:
        df: OHLCV verilerini içeren DataFrame
        period: Breakout periyodu (default: 20)
        column: Hesaplanacak sütun (default: "close")
    
    Returns:
        Breakout sinyallerini içeren pandas Series (1: long, -1: short, 0: no signal)
    """
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([0] * len(df), index=df.index)
        
        price_series = safe_column_access(df, column)
        
        if len(price_series) < period + 1:
            logger.warning(f"Breakout: Not enough data points ({len(price_series)} < {period + 1})")
            return pd.Series([0] * len(df), index=df.index)
        
        rolling_high = price_series.rolling(window=period).max()
        rolling_low = price_series.rolling(window=period).min()
        
        cond_long = price_series > rolling_high.shift(1)
        cond_short = price_series < rolling_low.shift(1)
        
        signal = pd.Series(0, index=price_series.index, dtype=float)
        signal[cond_long] = 1.0   # Long breakout
        signal[cond_short] = -1.0 # Short breakout
        
        return ta_circuit_breaker.execute(lambda: signal)
    except Exception as e:
        logger.error(f"Breakout hesaplanırken hata: {e}")
        return pd.Series([0] * len(df), index=df.index)

# =============================================================
# Sentiment İndikatörleri - TAMAMEN GÜNCELLENMİŞ
# =============================================================

@track_performance
async def funding_rate_placeholder_async(symbol: str = "BTCUSDT") -> float:
    """
    Asenkron funding rate placeholder (gerçek API ile değiştir).
    
    Args:
        symbol: Sembol adı (default: "BTCUSDT")
    
    Returns:
        Funding rate değeri
    """
    try:
        await asyncio.sleep(0.05)  # Simüle edilmiş IO işlemi
        logger.debug(f"Funding rate placeholder called for {symbol}")
        return 0.001  # Varsayılan değer
    except Exception as e:
        logger.error(f"Funding rate placeholder error: {e}")
        return 0.001

@track_performance
async def social_sentiment_placeholder_async(symbol: str = "BTCUSDT") -> Dict[str, float]:
    """
    Asenkron sosyal sentiment placeholder (gerçek API ile değiştir).
    
    Args:
        symbol: Sembol adı (default: "BTCUSDT")
    
    Returns:
        Sosyal sentiment bilgilerini içeren dictionary
    """
    try:
        await asyncio.sleep(0.05)  # Simüle edilmiş IO işlemi
        logger.debug(f"Social sentiment placeholder called for {symbol}")
        return {"sentiment": 0.5, "bullish": 0.6, "bearish": 0.4}  # Varsayılan değerler
    except Exception as e:
        logger.error(f"Social sentiment placeholder error: {e}")
        return {"sentiment": 0.5, "bullish": 0.5, "bearish": 0.5}

@track_performance
def funding_rate_placeholder(symbol: str = "BTCUSDT") -> float:
    """
    Senkron funding rate placeholder (registry kullanım kolaylığı için).
    
    Args:
        symbol: Sembol adı (default: "BTCUSDT")
    
    Returns:
        Funding rate değeri
    """
    try:
        logger.debug(f"Funding rate sync placeholder called for {symbol}")
        return 0.001  # Varsayılan değer
    except Exception as e:
        logger.error(f"Funding rate sync placeholder error: {e}")
        return 0.001

@track_performance
def social_sentiment_placeholder(symbol: str = "BTCUSDT") -> Dict[str, float]:
    """
    Senkron sosyal sentiment placeholder (registry kullanım kolaylığı için).
    
    Args:
        symbol: Sembol adı (default: "BTCUSDT")
    
    Returns:
        Sosyal sentiment bilgilerini içeren dictionary
    """
    try:
        logger.debug(f"Social sentiment sync placeholder called for {symbol}")
        return {"sentiment": 0.5, "bullish": 0.6, "bearish": 0.4}  # Varsayılan değerler
    except Exception as e:
        logger.error(f"Social sentiment sync placeholder error: {e}")
        return {"sentiment": 0.5, "bullish": 0.5, "bearish": 0.5}

# =============================================================
# Registry - GÜNCELLENMİŞ
# =============================================================

# CPU-bound olarak toplu hesaplanacak fonksiyonlar
CPU_FUNCTIONS = {
    "ema": ema,
    "macd": macd,
    "adx": adx,
    "vwap": vwap,
    "cci": cci,
    "momentum": momentum,
    "rsi": rsi,
    "stochastic": stochastic,
    "atr": atr,
    "bollinger_bands": bollinger_bands,
    "sharpe_ratio": sharpe_ratio,
    "max_drawdown": max_drawdown,
    "historical_volatility": historical_volatility,
    "ulcer_index": ulcer_index,
    "obv": obv,
    "cmf": cmf,
    "market_structure": market_structure,
    "breakout": breakout,
}

# Paralelde çalıştırılırken df'yi mutate eden (yan etki oluşturan) fonksiyonlar
MUTATING_FUNCTIONS = {"adx"}  # ADX TR, +DM, -DM kolonları ekler

# I/O-bound asenkron fonksiyonlar (gerçek API'lerle değiştirilebilir)
IO_FUNCTIONS = {
    "funding_rate": funding_rate_placeholder_async,
    "social_sentiment": social_sentiment_placeholder_async,
}

# Genel amaçlı string-çağrı registry (senkron kullanımlar için)
TA_FUNCTIONS = {
    **CPU_FUNCTIONS,
    "order_book_imbalance": order_book_imbalance,
    "open_interest_placeholder": open_interest_placeholder,
    "funding_rate_placeholder": funding_rate_placeholder,
    "social_sentiment_placeholder": social_sentiment_placeholder,
}

# =============================================================
# Hibrit Pipeline - GÜNCELLENMİŞ
# =============================================================

@track_performance
def calculate_cpu_functions(df: pd.DataFrame, max_workers: Optional[int] = None) -> dict:
    """
    CPU-bound fonksiyonları paralelde hesaplar.
    - Free Render için default max_workers=2 (CONFIG.SYSTEM.MAX_WORKERS yoksa).
    - MUTATING_FUNCTIONS için df.copy() ile izole çalıştırır.
    """
    results: dict = {}
    max_workers = max_workers or _get_max_workers(default=2)

    # Hafıza ve stabilite açısından Free Render'da 2 önerilir.
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for name, func in CPU_FUNCTIONS.items():
            # Yan etki oluşturan fonksiyonlara izolasyon
            arg_df = df.copy(deep=True) if name in MUTATING_FUNCTIONS else df
            futures[executor.submit(func, arg_df)] = name

        for future in as_completed(futures):
            name = futures[future]
            try:
                results[name] = future.result()
                ta_metrics.total_calculations += 1
            except Exception as e:
                results[name] = None
                ta_metrics.calculation_errors += 1
                logger.error(f"[CPU TA ERROR] {name} hesaplanamadı: {e}")
    return results

@track_performance
async def calculate_io_functions(symbol: str = "default") -> dict:
    """
    I/O-bound (asenkron) fonksiyonları birlikte yürütür.
    """
    results: dict = {}
    names = list(IO_FUNCTIONS.keys())
    tasks = [IO_FUNCTIONS[n](symbol) for n in names]
    completed = await asyncio.gather(*tasks, return_exceptions=True)
    for name, res in zip(names, completed):
        if isinstance(res, Exception):
            results[name] = None
            ta_metrics.io_errors += 1
            logger.error(f"[I/O TA ERROR] {name} hesaplanamadı: {res}")
        else:
            results[name] = res
            ta_metrics.io_requests += 1
    return results

def _run_asyncio(coro):
    """
    Jupyter / Bot / Web server ortamlarında güvenli asyncio çalıştırma.
    - Çalışan bir loop varsa yeni loop açar.
    """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # yeni event loop oluştur ve orada çalıştır
            new_loop = asyncio.new_event_loop()
            try:
                asyncio.set_event_loop(new_loop)
                return new_loop.run_until_complete(coro)
            finally:
                new_loop.close()
                asyncio.set_event_loop(loop)
        else:
            return loop.run_until_complete(coro)
    except RuntimeError:
        # loop yoksa
        new_loop = asyncio.new_event_loop()
        try:
            asyncio.set_event_loop(new_loop)
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
            asyncio.set_event_loop(None)

@track_performance
def calculate_all_ta_hybrid(df: pd.DataFrame, symbol: str = "default", 
                          max_workers: Optional[int] = None) -> dict:
    """
    Tüm TA'leri hibrit olarak hesaplar:
      - CPU-bound: ThreadPoolExecutor
      - I/O-bound: asyncio
    Dönen sonuç: { indicator_name: value_or_series_or_df }
    """
    cpu_results = calculate_cpu_functions(df, max_workers=max_workers)
    io_results = _run_asyncio(calculate_io_functions(symbol))
    return {**cpu_results, **io_results}

# =============================================================
# Advanced Signal Stack: alpha_ta - ESKİ KODDAN AKTARILDI ve GELİŞTİRİLDİ
# =============================================================

# --- 1) Kalman:
# Güncellenmiş Kalman filter (fillna düzeltmesi)
@track_performance
def kalman_filter_series(prices: pd.Series, q: Optional[float] = None, r: Optional[float] = None) -> pd.Series:
    """Minimal 1D random-walk Kalman."""
    try:
        if q is None:
            q = getattr(CONFIG.TA, "KALMAN_Q", 1e-5)
        if r is None:
            r = getattr(CONFIG.TA, "KALMAN_R", 1e-2)

        x = 0.0
        p = 1.0
        out = []
        initialized = False

        # fillna düzeltmesi
        vals = prices.ffill().values
        for z in vals:
            z = float(z)
            if not initialized:
                x = z
                initialized = True

            # predict
            x_prior = x
            p_prior = p + q

            # update
            k = p_prior / (p_prior + r)
            x = x_prior + k * (z - x_prior)
            p = (1 - k) * p_prior
            out.append(x)
        return pd.Series(out, index=prices.index, name="kalman")
    except Exception as e:
        logger.error(f"Kalman filter error: {e}")
        return pd.Series([np.nan] * len(prices), index=prices.index)

# --- 2) Hilbert:
@track_performance
def _hilbert_fallback(x: np.ndarray) -> np.ndarray:
    """Hilbert transform fallback implementation"""
    n = len(x)
    Xf = np.fft.fft(x)
    h = np.zeros(n)
    if n % 2 == 0:
        h[0] = 1; h[n//2] = 1; h[1:n//2] = 2
    else:
        h[0] = 1; h[1:(n+1)//2] = 2
    return np.fft.ifft(Xf * h)

# Güncellenmiş Hilbert features (fillna düzeltmesi)
@track_performance
def hilbert_features(prices: pd.Series) -> dict:
    """Hilbert transform features"""
    try:
        # fillna düzeltmesi
        x = prices.ffill().values.astype(float)
        try:
            from scipy.signal import hilbert
            analytic = hilbert(x)
        except Exception:
            analytic = _hilbert_fallback(x)
        amp = np.abs(analytic)
        phase = np.unwrap(np.angle(analytic))
        inst_freq = np.diff(phase, prepend=phase[0])
        return {
            "amp": pd.Series(amp, index=prices.index),
            "inst_freq": pd.Series(inst_freq, index=prices.index),
        }
    except Exception as e:
        logger.error(f"Hilbert features error: {e}")
        return {
            "amp": pd.Series([np.nan] * len(prices), index=prices.index),
            "inst_freq": pd.Series([np.nan] * len(prices), index=prices.index),
        }

# --- 3) Entropy measures
@track_performance
def _phi(m: int, r: float, series: np.ndarray) -> float:
    """Entropy calculation helper"""
    n = len(series)
    if n <= m + 1:
        return np.inf
    x = np.array([series[i:i+m] for i in range(n - m + 1)])
    # pairwise Chebyshev distances
    d = np.max(np.abs(x[:, None, :] - x[None, :, :]), axis=2)
    C = (d <= r).sum(axis=1) / (n - m + 1)
    C = C[C > 0]
    return np.sum(np.log(C)) / (len(C) + 1e-12) if len(C) else np.inf

@track_performance
def approximate_entropy(series: pd.Series, m: Optional[int] = None, r: Optional[float] = None) -> float:
    """Approximate entropy calculation"""
    try:
        s = series.dropna().values.astype(float)
        if len(s) < 5:
            return np.nan
        m = getattr(CONFIG.TA, "ENTROPY_M", 3) if m is None else m
        if r is None:
            r = getattr(CONFIG.TA, "ENTROPY_R_FACTOR", 0.2) * np.std(s)
        return float(_phi(m, r, s) - _phi(m+1, r, s))
    except Exception as e:
        logger.error(f"Approximate entropy error: {e}")
        return np.nan

@track_performance
def sample_entropy(series: pd.Series, m: Optional[int] = None, r: Optional[float] = None) -> float:
    """Sample entropy calculation"""
    try:
        s = series.dropna().values.astype(float)
        if len(s) < 5:
            return np.nan
        m = getattr(CONFIG.TA, "ENTROPY_M", 3) if m is None else m
        if r is None:
            r = getattr(CONFIG.TA, "ENTROPY_R_FACTOR", 0.2) * np.std(s)
        n = len(s)
        xm = np.array([s[i:i+m] for i in range(n-m)])
        xm1 = np.array([s[i:i+m+1] for i in range(n-m-1)])
        def count_sim(x, tol):
            d = np.max(np.abs(x[:, None, :] - x[None, :, :]), axis=2)
            return (d <= tol).sum() - len(x)
        B = count_sim(xm, r)
        A = count_sim(xm1, r)
        if B <= 0 or A <= 0:
            return np.nan
        return float(-np.log(A / B))
    except Exception as e:
        logger.error(f"Sample entropy error: {e}")
        return np.nan

@track_performance
def permutation_entropy(series: pd.Series, m: Optional[int] = None) -> float:
    """Permutation entropy calculation"""
    try:
        s = series.dropna().values.astype(float)
        m = 3 if m is None else m
        if len(s) < m:
            return np.nan
        patterns = {}
        for i in range(len(s) - m + 1):
            pat = tuple(np.argsort(s[i:i+m]))
            patterns[pat] = patterns.get(pat, 0) + 1
        p = np.array(list(patterns.values()), dtype=float)
        p /= p.sum()
        return float(-np.sum(p * np.log(p + 1e-12)) / np.log(math.factorial(m)))
    except Exception as e:
        logger.error(f"Permutation entropy error: {e}")
        return np.nan

# --- 4) Rejim tespiti 
@track_performance
def detect_regime(df: pd.DataFrame, window: Optional[int] = None) -> pd.Series:
    """
    Basit rejim skoru: trendiness ~ trend_z - penalty(vol_z)
    Çıktı: [-1,1] aralığına yakın normalize bir seri.
    """
    try:
        window = getattr(CONFIG.TA, "REGIME_WINDOW", 80) if window is None else window
        px = safe_column_access(df, "close").astype(float)
        if len(px) < window + 5:
            return pd.Series([0.0] * len(px), index=px.index, name="regime_score")
        ret = px.pct_change()
        vol = ret.rolling(window).std()
        # pencere içinde lineer trend eğimi
        def slope(x):
            idx = np.arange(len(x))
            b, a = np.polyfit(idx, x, 1)
            return b
        trend = px.rolling(window).apply(slope, raw=True)
        # normalize (robust-ish)
        def zscore(s):
            m = s.rolling(window).mean()
            sd = s.rolling(window).std()
            return (s - m) / (sd + 1e-9)
        trend_z = zscore(trend).fillna(0).clip(-3,3)/3.0
        vol_z = zscore(vol).fillna(0).clip(-3,3)/3.0
        score = trend_z - 0.5*np.maximum(vol_z-0.5, 0.0)
        return score.fillna(0.0).rename("regime_score")
    except Exception as e:
        logger.error(f"Regime detection error: {e}")
        px = safe_column_access(df, "close")
        return pd.Series([0.0] * len(px), index=px.index, name="regime_score")

# --- 5) Lead-Lag: max xcorr lag
@track_performance
def leadlag_xcorr(target: pd.Series, reference: pd.Series, max_lag: Optional[int] = None) -> dict:
    """
    target ve reference getirileri arasında [-max_lag, max_lag] gecikmede
    maksimum korelasyonu bulur. Skoru [-1,1]'e sıkıştırır.
    """
    try:
        max_lag = getattr(CONFIG.TA, "LEADLAG_MAX_LAG", 10) if max_lag is None else max_lag
        x = target.pct_change().dropna().values
        y = reference.pct_change().dropna().values
        L = min(len(x), len(y))
        if L < max_lag + 5:
            return {"lag": 0, "corr": 0.0, "score": 0.0}
        x = x[-L:]; y = y[-L:]
        best_corr, best_lag = 0.0, 0
        for lag in range(-max_lag, max_lag+1):
            if lag < 0:
                corr = np.corrcoef(x[:lag], y[-lag:])[0,1]
            elif lag > 0:
                corr = np.corrcoef(x[lag:], y[:-lag])[0,1]
            else:
                corr = np.corrcoef(x, y)[0,1]
            if np.isfinite(corr) and abs(corr) > abs(best_corr):
                best_corr, best_lag = float(corr), lag
        score = float(np.tanh(best_corr))  # [-1,1]
        return {"lag": int(best_lag), "corr": float(best_corr), "score": score}
    except Exception as e:
        logger.error(f"Lead-lag correlation error: {e}")
        return {"lag": 0, "corr": 0.0, "score": 0.0}

# --- 6) Birleşik skorlayıcı + sinyal
@track_performance
def compute_alpha_ta(df: pd.DataFrame, ref_series: Optional[pd.Series] = None) -> dict:
    """
    Döndürür:
      {
        "score": float in [-1,1],
        "detail": {...},
        "series": {"kalman": Series, "regime_score": Series}
      }
    """
    try:
        px = safe_column_access(df, "close").astype(float)
        # Kalman
        kf = kalman_filter_series(px)
        kf_err = (px - kf).rolling(20).std()
        kf_score = float(np.tanh((kf.diff().iloc[-1]) / (float(kf_err.iloc[-1]) + 1e-9))) if len(kf) > 21 else 0.0

        # Hilbert
        h = hilbert_features(px)
        hf = h["inst_freq"].rolling(10).mean()
        ha = h["amp"].rolling(10).mean()
        hilbert_raw = (hf.diff().iloc[-1] if len(hf) > 1 else 0.0)
        hilbert_penalty = float(np.tanh((ha.pct_change().rolling(10).std().iloc[-1] if len(ha) > 11 else 0.0)))
        hilbert_score = float(np.tanh(hilbert_raw)) * (1.0 - 0.3*abs(hilbert_penalty))

        # Entropy
        apen = approximate_entropy(px)
        samp = sample_entropy(px)
        pent = permutation_entropy(px)
        ent_vals = np.array([v for v in [apen, samp, pent] if isinstance(v, (int,float)) and np.isfinite(v)])
        if len(ent_vals):
            ent_z = (ent_vals - np.nanmean(ent_vals)) / (np.nanstd(ent_vals) + 1e-9)
            entropy_score = float(np.tanh(-ent_z.mean()))  # düşük entropy → pozitif
        else:
            entropy_score = 0.0

        # Rejim
        reg = detect_regime(df)
        regime_score = float(np.clip(reg.iloc[-1] if len(reg) else 0.0, -1.0, 1.0))

        # Lead-Lag (opsiyonel)
        if isinstance(ref_series, pd.Series) and len(ref_series) >= len(px)//2:
            ll = leadlag_xcorr(px, ref_series)
            leadlag_score = ll["score"]
            leadlag_detail = ll
        else:
            leadlag_score = 0.0
            leadlag_detail = {"lag": 0, "corr": 0.0, "score": 0.0}

        # Ağırlıklı birleşim
        w = CONFIG.TA
        score = (
            getattr(w, "W_KALMAN", 0.20)   * kf_score +
            getattr(w, "W_HILBERT", 0.20)  * hilbert_score +
            getattr(w, "W_ENTROPY", 0.20)  * entropy_score +
            getattr(w, "W_REGIME", 0.20)   * regime_score +
            getattr(w, "W_LEADLAG", 0.20)  * leadlag_score
        )
        score = float(np.clip(score, -1.0, 1.0))

        return {
            "score": score,
            "detail": {
                "kalman_score": kf_score,
                "hilbert_score": hilbert_score,
                "entropy_score": entropy_score,
                "regime_score": regime_score,
                "leadlag": leadlag_detail,
            },
            "series": {
                "kalman": kf,
                "regime_score": reg,
            }
        }
    except Exception as e:
        logger.error(f"[ALPHA_TA ERROR] {e}")
        return {"score": 0.0, "detail": {}, "series": {}}

@track_performance
def alpha_signal(df: pd.DataFrame, ref_series: Optional[pd.Series] = None) -> dict:
    """Alpha signal generation"""
    res = compute_alpha_ta(df, ref_series=ref_series)
    s = res["score"]
    long_threshold = getattr(CONFIG.TA, "ALPHA_LONG_THRESHOLD", 0.6)
    short_threshold = getattr(CONFIG.TA, "ALPHA_SHORT_THRESHOLD", -0.6)
    
    if s >= long_threshold:
        sig = 1
    elif s <= short_threshold:
        sig = -1
    else:
        sig = 0
    res["signal"] = sig
    return res

# =============================================================
# Registry Güncellemeleri alpha_ta/alpha_signal ekleri (CPU parallel'e uygun)
# =============================================================

# CPU-bound fonksiyonlara alpha_ta ekle
CPU_FUNCTIONS["alpha_ta"] = lambda df: compute_alpha_ta(df)
CPU_FUNCTIONS["alpha_signal"] = lambda df: alpha_signal(df)

# TA_FUNCTIONS registry güncellemesi
TA_FUNCTIONS = {
    **TA_FUNCTIONS,
    "alpha_ta": compute_alpha_ta,
    "alpha_signal": alpha_signal,
}

# =============================================================
# Generate Signals ve Scan Market Fonksiyonları - GÜNCELLENMİŞ
# =============================================================

@track_performance
def generate_signals(df: pd.DataFrame, ref_series: Optional[pd.Series] = None) -> dict:
    """
    Basit klasik TA kararı + alpha_ta sinyali beraber.
    """
    try:
        indicators: Dict[str, float] = {}

        # Trend: EMA - scalar değerleri explicit olarak al
        ema_periods = getattr(CONFIG.TA, "EMA_PERIODS", [20, 50])
        ema_fast = ema(df, period=ema_periods[0])
        ema_slow = ema(df, period=ema_periods[1])
        
        # Series değil, scalar değerler al (iloc[-1] kullanarak)
        ema_fast_val = float(ema_fast.iloc[-1]) if not ema_fast.empty and not pd.isna(ema_fast.iloc[-1]) else np.nan
        ema_slow_val = float(ema_slow.iloc[-1]) if not ema_slow.empty and not pd.isna(ema_slow.iloc[-1]) else np.nan
        
        indicators["ema_fast"] = ema_fast_val
        indicators["ema_slow"] = ema_slow_val
        
        # EMA sinyalini scalar değerlerle hesapla
        if pd.isna(ema_fast_val) or pd.isna(ema_slow_val):
            ema_signal = 0
        else:
            ema_signal = 1 if ema_fast_val > ema_slow_val else -1

        # MACD - scalar değer al
        macd_line, signal_line, _ = macd(df)
        macd_val = 0.0
        if not macd_line.empty and not signal_line.empty:
            macd_current = macd_line.iloc[-1] if not pd.isna(macd_line.iloc[-1]) else 0.0
            signal_current = signal_line.iloc[-1] if not pd.isna(signal_line.iloc[-1]) else 0.0
            macd_val = float(macd_current - signal_current)
        
        indicators["macd"] = macd_val
        macd_signal = 1 if macd_val > 0 else (-1 if macd_val < 0 else 0)

        # Momentum: RSI - scalar değer al
        rsi_series = rsi(df)
        rsi_val = 50.0
        if not rsi_series.empty and not pd.isna(rsi_series.iloc[-1]):
            rsi_val = float(rsi_series.iloc[-1])
        
        indicators["rsi"] = rsi_val
        rsi_signal = 1 if rsi_val < 30 else (-1 if rsi_val > 70 else 0)

        # Volatilite: ATR - scalar değer al
        atr_series = atr(df)
        atr_val = np.nan
        if not atr_series.empty and not pd.isna(atr_series.iloc[-1]):
            atr_val = float(atr_series.iloc[-1])
        indicators["atr"] = atr_val

        # Hacim: OBV - scalar değer al
        obv_series = obv(df)
        obv_val = 0.0
        if not obv_series.empty and not pd.isna(obv_series.iloc[-1]):
            obv_val = float(obv_series.iloc[-1])
        indicators["obv"] = obv_val
        
        # OBV sinyalini doğru hesapla
        obv_signal = 0
        if len(obv_series) > 20:
            obv_prev = float(obv_series.iloc[-20]) if not obv_series.empty and len(obv_series) > 20 and not pd.isna(obv_series.iloc[-20]) else 0.0
            obv_signal = 1 if obv_val > obv_prev else (-1 if obv_val < obv_prev else 0)
        else:
            obv_signal = 0

        weights = {"ema":0.3, "macd":0.3, "rsi":0.2, "obv":0.2}
        score = (ema_signal * weights["ema"] + 
                 macd_signal * weights["macd"] +
                 rsi_signal * weights["rsi"] + 
                 obv_signal * weights["obv"])

        # Eşikleri config'ten al
        long_threshold = getattr(CONFIG.TA, "ALPHA_LONG_THRESHOLD", 0.6)
        short_threshold = getattr(CONFIG.TA, "ALPHA_SHORT_THRESHOLD", -0.6)
        
        if score >= long_threshold:
            signal = 1
        elif score <= short_threshold:
            signal = -1
        else:
            signal = 0

        # alpha_ta
        alpha = alpha_signal(df, ref_series=ref_series)
        return {
            "signal": signal,
            "score": round(float(score), 4),
            "indicators": indicators,
            "alpha_ta": alpha
        }
    except Exception as e:
        logger.error(f"[SIGNAL ERROR] Sinyal hesaplanamadı: {e}")
        return {"signal": 0, "score": 0.0, "indicators": {}, "alpha_ta": {"score": 0.0, "signal": 0}}

@track_performance
def scan_market(market_data: Dict[str, pd.DataFrame], ref_close: Optional[pd.Series] = None) -> dict:
    """
    Çoklu sembol taraması:
      market_data: { "BTCUSDT": df, "ETHUSDT": df, ... }
      ref_close  : opsiyonel referans seri (örn. BTC close) lead-lag için
    """
    results: Dict[str, dict] = {}
    for symbol, df in market_data.items():
        if not isinstance(df, pd.DataFrame) or df.empty:
            results[symbol] = {"alpha_ta": {"score": 0.0, "signal": 0}}
            continue
        try:
            results[symbol] = alpha_signal(df, ref_series=ref_close)
        except Exception as e:
            logger.error(f"[SCAN ERROR] {symbol}: {e}")
            results[symbol] = {"alpha_ta": {"score": 0.0, "signal": 0}}
    return results

# =============================================================
# Binance Entegre IO Fonksiyonları - GELİŞMİŞ CACHE
# =============================================================

@track_performance
async def fetch_funding_rate_binance(symbol: str = "BTCUSDT") -> float:
    """
    Binance'den gerçek funding rate çeker.
    
    Args:
        symbol: Sembol adı (default: "BTCUSDT")
    
    Returns:
        Funding rate değeri
    """
    symbol = normalize_symbol(symbol)
    cache_key = f"funding_rate_{symbol}"
    
    # Önce cache kontrolü
    cached = ta_cache.get_ta_result(symbol, "funding_rate", "funding_rate")
    if cached is not None:
        ta_metrics.cache_hits += 1
        return cached
    
    ta_metrics.cache_misses += 1
    ta_metrics.io_requests += 1
    
    try:
        from utils.binance_api import get_global_binance_client
        client = await get_global_binance_client()
        
        if client is None:
            logger.warning("Binance client not available for funding rate")
            result = 0.001
            ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=60)
            return result
            
        try:
            await client.load_markets()
            
            futures_symbol = symbol.replace('/', '').replace(':', '').upper()
            if futures_symbol not in client.markets:
                logger.warning(f"Symbol {futures_symbol} not found in futures markets")
                result = 0.001
                ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=300)
                return result
            
            market = client.markets[futures_symbol]
            
            if market.get('linear', False) or market.get('inverse', False):
                funding_data = await client.fetch_funding_rate(symbol)
                result = float(funding_data['fundingRate']) if funding_data else 0.001
                ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=300)
                return result
            else:
                logger.warning(f"Symbol {futures_symbol} is not a linear or inverse contract")
                result = 0.001
                ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=600)
                return result
                
        except Exception as e:
            ta_metrics.io_errors += 1
            if "authentication" in str(e).lower():
                logger.warning(f"Funding rate için yetki yetersiz: {e}")
            elif "not supported" in str(e).lower():
                logger.warning(f"Funding rate desteklenmiyor: {e}")
            else:
                logger.error(f"Funding rate çekme hatası: {e}")
            result = 0.001
            ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=60)
            return result
                
    except Exception as e:
        ta_metrics.io_errors += 1
        logger.error(f"Funding rate çekilemedi: {e}")
        result = 0.001
        ta_cache.set_ta_result(symbol, "funding_rate", "funding_rate", result, ttl=30)
        return result

# =============================================================
# Trading Pipeline - GELİŞMİŞ HATA YÖNETİMİ
# =============================================================

@track_performance
async def optimized_trading_pipeline(symbol: str = "BTCUSDT", 
                                   interval: str = "1m",
                                   callback: Optional[Callable] = None,
                                   max_retries: int = 3):
    """
    Geliştirilmiş trading pipeline with enhanced error handling and retry.
    
    Args:
        symbol: İşlem yapılacak sembol (default: "BTCUSDT")
        interval: Zaman aralığı (default: "1m")
        callback: Sonuçları işleyecek callback fonksiyonu
        max_retries: Maksimum yeniden deneme sayısı (default: 3)
    """
    pipeline_logger = logging.getLogger(f"ta_pipeline.{symbol}")
    
    try:
        from utils.binance_api import get_binance_client
        client = get_binance_client(None, None)
    except ImportError as e:
        pipeline_logger.error(f"Binance API modülü yüklenemedi: {e}")
        return
    
    pipeline_interval = getattr(CONFIG.TA, 'TA_PIPELINE_INTERVAL', 60)
    min_data_points = getattr(CONFIG.TA, 'TA_MIN_DATA_POINTS', 20)
    cache_ttl = getattr(CONFIG.TA, 'TA_CACHE_TTL', 300)
    
    retry_count = 0
    retry_delay = 5
    
    while True:
        try:
            # Cache kontrolü
            cached_result = ta_cache.get_ta_result(symbol, interval, 'full_analysis')
            
            if cached_result:
                pipeline_logger.debug("Using cached TA results")
                if callback:
                    try:
                        await callback(cached_result)
                    except Exception as e:
                        pipeline_logger.error(f"Callback error: {e}")
                await asyncio.sleep(pipeline_interval)
                continue
            
            # Veri çekme
            klines = await client.get_klines(symbol, interval, limit=100)
            if not klines or len(klines) < min_data_points:
                pipeline_logger.warning(f"Insufficient kline data: {len(klines) if klines else 0} points")
                await asyncio.sleep(30)
                continue
            
            # DataFrame dönüşümü
            df = klines_to_dataframe(klines)
            
            # DataFrame validation
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            if not validate_dataframe(df, required_columns):
                pipeline_logger.error("Invalid DataFrame format")
                await asyncio.sleep(30)
                continue
            
            if len(df) < min_data_points:
                pipeline_logger.warning(f"Insufficient data points: {len(df)} < {min_data_points}")
                await asyncio.sleep(30)
                continue
            
            # TA hesaplama
            try:
                ta_results = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: calculate_all_ta_hybrid(df, symbol)
                )
            except Exception as e:
                pipeline_logger.error(f"TA calculation failed: {e}")
                await asyncio.sleep(30)
                continue
            
            # Sinyal üretme
            try:
                signal = generate_signals(df)
            except Exception as e:
                pipeline_logger.error(f"Signal generation failed: {e}")
                signal = {'signal': 0, 'signals': {}, 'total_signal': 0.0, 'alpha_details': {}}
            
            # Sonuç paketleme
            result = {
                'symbol': symbol,
                'timestamp': time.time(),
                'price': float(safe_column_access(df, 'close').iloc[-1]),
                'signal': signal,
                'ta_metrics': {k: float(v.iloc[-1]) if hasattr(v, 'iloc') and not v.empty else v 
                              for k, v in ta_results.items() if v is not None}
            }
            
            # Cache'e kaydetme
            ta_cache.set_ta_result(symbol, interval, 'full_analysis', result, ttl=cache_ttl)
            
            # Callback
            if callback:
                try:
                    await callback(result)
                except Exception as e:
                    pipeline_logger.error(f"Callback execution failed: {e}")
            
            # Başarılı oldu, retry sıfırla
            retry_count = 0
            retry_delay = 5
            
            await asyncio.sleep(pipeline_interval)
            
        except asyncio.CancelledError:
            pipeline_logger.info("Trading pipeline cancelled")
            break
        except Exception as e:
            retry_count += 1
            pipeline_logger.error(f"Trading pipeline error (retry {retry_count}/{max_retries}): {e}")
            
            if retry_count >= max_retries:
                pipeline_logger.error("Max retries exceeded, stopping pipeline")
                break
                
            await asyncio.sleep(retry_delay)
            retry_delay = min(retry_delay * 2, 60)  # Exponential backoff

# =============================================================
# Gelişmiş Metrikler ve Monitoring
# =============================================================

def get_detailed_metrics() -> Dict[str, Any]:
    """Detaylı sistem metriklerini getir"""
    cache_stats = ta_cache.get_stats()
    circuit_status = ta_circuit_breaker.get_status()
    
    # Ortalama execution time'ları hesapla
    avg_times = {}
    for func_name, times in ta_metrics.execution_times.items():
        if times:
            avg_times[func_name] = sum(times) / len(times)
    
    return {
        'calculations': {
            'total': ta_metrics.total_calculations,
            'errors': ta_metrics.calculation_errors,
            'error_rate': ta_metrics.calculation_errors / max(ta_metrics.total_calculations, 1),
            'cache_hits': ta_metrics.cache_hits,
            'cache_misses': ta_metrics.cache_misses,
            'cache_hit_ratio': cache_stats['hit_ratio']
        },
        'io': {
            'requests': ta_metrics.io_requests,
            'errors': ta_metrics.io_errors,
            'error_rate': ta_metrics.io_errors / max(ta_metrics.io_requests, 1)
        },
        'performance': {
            'average_times': avg_times,
            'monitored_functions': list(ta_metrics.execution_times.keys())
        },
        'cache': cache_stats,
        'circuit_breaker': circuit_status,
        'timestamp': time.time(),
        'status': 'healthy' if (ta_metrics.calculation_errors / max(ta_metrics.total_calculations, 1)) < 0.1 else 'degraded'
    }

def reset_metrics():
    """Metrikleri sıfırla"""
    global ta_metrics
    ta_metrics = TAMetrics()
    logger.info("Metrics reset")

# =============================================================
# Health Check ve Monitoring
# =============================================================

def health_check() -> Dict[str, Any]:
    """Sistem sağlık durumunu kontrol et"""
    metrics = get_detailed_metrics()
    
    health_status = {
        "status": "healthy",
        "timestamp": time.time(),
        "components": {
            "calculations": "healthy" if metrics['calculations']['error_rate'] < 0.1 else "degraded",
            "io": "healthy" if metrics['io']['error_rate'] < 0.2 else "degraded",
            "cache": "healthy" if metrics['cache']['hit_ratio'] > 0.3 else "degraded",
            "circuit_breaker": metrics['circuit_breaker']['state'].lower()
        },
        "metrics": metrics
    }
    
    # Genel durumu belirle
    if (metrics['calculations']['error_rate'] > 0.3 or 
        metrics['io']['error_rate'] > 0.5 or
        metrics['circuit_breaker']['state'] == CircuitState.OPEN.value):
        health_status["status"] = "degraded"
    
    return health_status

# =============================================================
# Unit Test Güncellemeleri
# =============================================================

def run_unit_tests():
    """Tüm fonksiyonlar için unit testleri çalıştır"""
    logger.info("Running unit tests...")
    
    # Test verisi
    test_data = {
        'open': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],
        'high': [105, 106, 107, 108, 109, 110, 111, 112, 113, 114],
        'low': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104],
        'close': [102, 103, 104, 105, 106, 107, 108, 109, 110, 111],
        'volume': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]
    }
    df = pd.DataFrame(test_data)
    
    # Test fonksiyonları
    test_functions = [
        (ema, (df,), {}),
        (lambda df: macd(df)[0], (df,), {}),  # MACD line only
        (rsi, (df,), {}),
        (lambda df: stochastic(df)[0], (df,), {}),  # Stochastic K only
    ]
    
    passed = 0
    failed = 0
    
    for func, args, kwargs in test_functions:
        try:
            result = func(*args, **kwargs)
            # Basit validation
            if result is not None and not (hasattr(result, 'isna') and result.isna().all()):
                passed += 1
                logger.info(f"✓ {func.__name__} test passed")
            else:
                failed += 1
                logger.warning(f"✗ {func.__name__} test failed: Invalid result")
        except Exception as e:
            failed += 1
            logger.error(f"✗ {func.__name__} test failed with error: {e}")
    
    logger.info(f"Unit tests completed: {passed} passed, {failed} failed")
    return passed, failed

def run_alpha_tests():
    """Alpha TA fonksiyonları için unit testler"""
    logger.info("Running Alpha TA unit tests...")
    
    # Test verisi
    test_data = {
        'open': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109] * 10,
        'high': [105, 106, 107, 108, 109, 110, 111, 112, 113, 114] * 10,
        'low': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104] * 10,
        'close': [102, 103, 104, 105, 106, 107, 108, 109, 110, 111] * 10,
        'volume': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000] * 10
    }
    df = pd.DataFrame(test_data)
    
    alpha_tests = [
        (kalman_filter_series, (df['close'],), {}),
        (hilbert_features, (df['close'],), {}),
        (approximate_entropy, (df['close'],), {}),
        (detect_regime, (df,), {}),
        (compute_alpha_ta, (df,), {}),
        (alpha_signal, (df,), {}),
        (generate_signals, (df,), {})
    ]
    
    passed = 0
    failed = 0
    
    for func, args, kwargs in alpha_tests:
        try:
            result = func(*args, **kwargs)
            # Basit validation
            if result is not None:
                passed += 1
                logger.info(f"✓ {func.__name__} test passed")
            else:
                failed += 1
                logger.warning(f"✗ {func.__name__} test failed: Invalid result")
        except Exception as e:
            failed += 1
            logger.error(f"✗ {func.__name__} test failed with error: {e}")
    
    logger.info(f"Alpha TA tests completed: {passed} passed, {failed} failed")
    return passed, failed

# Ana fonksiyona alpha testleri ekle
async def main():
    """Güncellenmiş ana test fonksiyonu"""
    logger.info("Starting comprehensive TA utils test with Alpha TA...")
    
    # Unit testleri çalıştır
    passed, failed = run_unit_tests()
    alpha_passed, alpha_failed = run_alpha_tests()
    
    total_passed = passed + alpha_passed
    total_failed = failed + alpha_failed
    
    if total_failed > 0:
        logger.warning(f"{total_failed} tests failed, proceeding with caution")
    
    # Test verisi
    test_data = {
        'open': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109] * 10,
        'high': [105, 106, 107, 108, 109, 110, 111, 112, 113, 114] * 10,
        'low': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104] * 10,
        'close': [102, 103, 104, 105, 106, 107, 108, 109, 110, 111] * 10,
        'volume': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000] * 10
    }
    df = pd.DataFrame(test_data)
    
    try:
        # Alpha TA test
        alpha_result = compute_alpha_ta(df)
        logger.info(f"Alpha TA score: {alpha_result.get('score', 0.0):.4f}")
        
        # Signal generation test
        signal = generate_signals(df)
        logger.info(f"Generated signal: {signal['signal']}, score: {signal['score']:.4f}")
        
        # Health check
        health = health_check()
        logger.info(f"Health status: {health['status']}")
        
    except Exception as e:
        logger.error(f"Alpha TA test failed: {e}")
        return False
    
    logger.info("All Alpha TA tests completed successfully!")
    return total_failed == 0

# =============================================================
# Ana Fonksiyon - GELİŞMİŞ TEST
# =============================================================

async def main():
    """Gelişmiş test fonksiyonu"""
    logger.info("Starting comprehensive TA utils test...")
    
    # Unit testleri çalıştır
    passed, failed = run_unit_tests()
    
    if failed > 0:
        logger.warning(f"{failed} unit tests failed, proceeding with caution")
    
    # Test verisi
    test_data = {
        'open': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109] * 10,
        'high': [105, 106, 107, 108, 109, 110, 111, 112, 113, 114] * 10,
        'low': [95, 96, 97, 98, 99, 100, 101, 102, 103, 104] * 10,
        'close': [102, 103, 104, 105, 106, 107, 108, 109, 110, 111] * 10,
        'volume': [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000] * 10
    }
    df = pd.DataFrame(test_data)
    
    # 1. DataFrame validation test
    logger.info("Testing DataFrame validation...")
    is_valid = validate_dataframe(df)
    logger.info(f"DataFrame validation: {is_valid}")
    
    # 2. TA hesaplama test
    logger.info("Testing TA calculations...")
    try:
        results = calculate_all_ta_hybrid(df, "BTCUSDT")
        logger.info(f"Calculated {len(results)} TA indicators")
        
        # 3. Sinyal üretme test
        signal = generate_signals(df)
        logger.info(f"Generated signal: {signal['signal']}")
        
        # 4. Health check
        health = health_check()
        logger.info(f"Health status: {health['status']}")
        
        # 5. Metrikler
        metrics = get_detailed_metrics()
        logger.info(f"System metrics: {metrics}")
        
        # 6. Circuit breaker status
        circuit_status = ta_circuit_breaker.get_status()
        logger.info(f"Circuit breaker status: {circuit_status['state']}")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        return False
    
    logger.info("All tests completed successfully!")
    return True

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)


@track_performance
def ema(df: pd.DataFrame, period: Optional[int] = None, column: str = "close") -> pd.Series:
    """Exponential Moving Average hesaplar."""
    try:
        if not validate_dataframe(df, [column]):
            return pd.Series([np.nan] * len(df), index=df.index)
        
        period = period or getattr(CONFIG.TA, 'EMA_PERIOD', 20)
        price_series = safe_column_access(df, column)
        
        if len(price_series) < period or price_series.isna().all():
            logger.warning(f"EMA: Not enough valid data points ({len(price_series)} < {period} or all NaN)")
            return pd.Series([np.nan] * len(df), index=df.index)
        
        return ta_circuit_breaker.execute(
            lambda: price_series.ewm(span=period, adjust=False).mean()
        )
    except Exception as e:
        logger.error(f"EMA hesaplanırken hata: {e}")
        return pd.Series([np.nan] * len(df), index=df.index if df is not None else [])
